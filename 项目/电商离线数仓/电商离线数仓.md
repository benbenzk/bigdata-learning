

## 基础环境搭建

### 配置IP和Host

1. hadoop01配置

   - ip配置

     ```
     vi /etc/sysconfig/network-scripts/ifcfg-ens33
     # 配置内容
     TYPE="Ethernet"
     PROXY_METHOD="none"
     BROWSER_ONLY="no"
     BOOTPROTO="static"
     DEFROUTE="yes"
     IPV4_FAILURE_FATAL="no"
     IPV6INIT="yes"
     IPV6_AUTOCONF="yes"
     IPV6_DEFROUTE="yes"
     IPV6_FAILURE_FATAL="no"
     IPV6_ADDR_GEN_MODE="stable-privacy"
     NAME="ens33"
     UUID="28284417-0e9a-47e1-ba61-753fe0add656"
     DEVICE="ens33"
     ONBOOT="yes"
     IPADDR=192.168.143.101
     NETMASK=255.255.255.0
     GATEWAY=192.168.143.1
     DNS1=192.168.143.1
     ```

     执行`service network restart`使ip配置生效

   - hostname配置

     ```
     vi /etc/hostname
     # 配置内容
     hadoop01
     ```

     重启系统生效

2. hadoop02~05参照hadoop01配置

### hosts映射配置

所有节点的`/etc/hosts`文件末尾追加配置

```
192.168.143.101 hadoop01
192.168.143.102 hadoop02
192.168.143.103 hadoop03
192.168.143.104 hadoop04
192.168.143.105 hadoop05
```

### selinux配置

在所有节点的`/etc/selinux/config`文件中修改配置

```
SELINUX=disabled
```

重启系统后生效

### 免密登陆配置

1. 所有集群节点生成密钥

   ```
   ssh-keygen -t rsa -P ""
   ```

2. 依次将hadoop02～05上的密钥文件同步到hadoop01上

   ```
   # hadoop02上执行
   scp id_rsa.pub root@hadoop01:/root/.ssh/id_rsa_hadoop02.pub
   # hadoop03上执行
   scp id_rsa.pub root@hadoop01:/root/.ssh/id_rsa_hadoop03.pub
   # hadoop04上执行
   scp id_rsa.pub root@hadoop01:/root/.ssh/id_rsa_hadoop04.pub
   # hadoop05上执行
   scp id_rsa.pub root@hadoop01:/root/.ssh/id_rsa_hadoop05.pub
   ```

   然后将所有密钥追加到authorized_keys文件

   ```
   cat id_rsa.pub >> authorized_keys
   cat id_rsa_hadoop02.pub >> authorized_keys
   cat id_rsa_hadoop03.pub >> authorized_keys
   cat id_rsa_hadoop04.pub >> authorized_keys
   cat id_rsa_hadoop05.pub >> authorized_keys
   ```

   最后将authorized_keys分别同步到hadoop02~05上

   ```
   scp authorized_keys root@hadoop02:/root/.ssh/
   scp authorized_keys root@hadoop03:/root/.ssh/
   scp authorized_keys root@hadoop04:/root/.ssh/
   scp authorized_keys root@hadoop05:/root/.ssh/
   ```

3. 验证

   所有节点执行

   ```
   ssh hadoop01
   ssh hadoop02
   ssh hadoop03
   ssh hadoop04
   ssh hadoop05
   ```

### 防火墙配置

```
# 所有节点执行
# 开启服务
systemctl start firewalld.service
# 开机启动
systemctl enable firewalld.service
# 查看防火墙状态
systemctl status firewalld.service
```

### 时间同步

1. 所有节点安装ntpd

   ```
   yum install ntp
   ```

2. hadoop01上修改配置

   ```
   restrict 192.168.143.0 mask 255.255.255.0 nomodify notrap
   #server 0.centos.pool.ntp.org iburst
   #server 1.centos.pool.ntp.org iburst
   #server 2.centos.pool.ntp.org iburst
   #server 3.centos.pool.ntp.org iburst
   server 127.127.1.0
   fudge 127.127.1.0 stratum 10
   ```

   配置`/etc/sysconfig/ntpd`，在文件末尾追加配置保证BOIS与系统时间同步

   ```
   SYNC_HWLOCK=yes
   ```

3. hadoop01上启动ntpd服务

   ```
   # 开启服务
   systemctl start ntpd.service
   # 开机启动服务
   systemctl enable ntpd.service
   #查看状态
   systemctl status ntpd.service
   ```

4. hadoop02～05上配置定时任务，同步hadoop01主机时间

   ```
   crontab -e
   # 配置
   2 * * * * ntpdate 192.168.143.101
   ```

### 脚本分发配置

1. 所有节点安装rsync

   ```sh
   yum install rsync
   ```

2. 脚本实现

   在hadoop01上

   ```sh
   cd /usr/local/bin/
   touch rsync-script
   chmod +x rsync-script
   ```

   脚本内容

   ```sh
   #!/bin/bash
   
   #1 获取输入参数个数，如果个数为0，直接退出
   paramnum=$#
   echo $paramnum
   if [ $paramnum -eq 0 ]
   then
   echo no params
   exit
   fi
   
   #2 根据传入参数获取文件名称
   p1=$1
   fname=`basename $p1`
   echo $fname
   
   #3 获取输入参数的绝对路径
   pdir=`cd -P $(dirname $p1); pwd`
   echo pdir=$pdir
   
   #4 获取用户名称
   user=`whoami`
   
   #5 循环执行rsync
   for((host=1; host<=5; host++))
   do
   echo ****** hadoop0$host ******
   hostname=hadoop0$host
   rsync -rvl $pdir/$fname $user@$hostname:$pdir
   done
   ```

   

### 安装JDK

在所有集群节点创建目录

```
# 软件目录
mkdir -p /opt/software
# 软件安装目录
mkdir -p /opt/servers
```

1. hadoop01上安装jdk

   ```
   tar -zxvf jdk-8u301-linux-x64.tar.gz -C /opt/servers/
   cd /opt/servers/
   mv jdk1.8.0_301/ jdk1.8
   ```

2. hadoop01上配置环境变量

   ```
   # 在/etc/profile末尾追加配置
   export JAVA_HOME=/opt/servers/jdk1.8
   export PATH=.:$PATH:$JAVA_HOME/bin
   ```

3. 从hadoop01同步jdk到其它集群节点

   ```
   rsync-script /opt/servers/jdk1.8
   rsync-script /etc/profile
   ```
   
4. 所有集群节点生效环境变量

   ```
   source /etc/profile
   ```

5. 验证

   ```
   java -version
   ```

## 分布式集群环境

技术选型

<table>
  <tbody>
    <tr>
      <td>Hadoop</td>
      <td>Hive</td>
      <td>Flume</td>
      <td>DataX</td>
      <td>Airflow</td>
      <td>Atlas</td>
      <td>Griffin</td>
      <td>Impala</td>
      <td>MySQL</td>
    </tr>
    <tr>
      <td>2.9.2</td>
      <td>2.3.7</td>
      <td>1.9</td>
      <td>3.0</td>
      <td>1.10</td>
      <td>1.2.0</td>
      <td>0.4.0</td>
      <td>impala-2.3.0-cdh5.5.0</td>
      <td>5.7</td>
    </tr>
  </tbody>
</table>

集群规划

<table>
  <thead>
    <th></th>
    <th>hadoop01</th><th>hadoop02</th><th>hadoop03</th><th>hadoop04</th><th>hadoop05</th>
  </thead>
  <tbody>
    <tr>
      <td>NameNode</td>
      <td>✔️</td><td></td><td></td><td></td><td></td>
    </tr>
    <tr>
      <td>SecondaryNameNode</td>
      <td></td><td>✔️</td><td></td><td></td><td></td>
    </tr>
    <tr>
      <td>DataNode</td>
      <td>✔️</td><td>✔️</td><td>✔️</td><td>✔️</td><td>✔️</td>
    </tr>
    <tr>
      <td>ResourceManager</td>
      <td>✔️</td><td></td><td></td><td></td><td></td>
    </tr>
    <tr>
      <td>DataManager</td>
      <td>✔️</td><td>✔️</td><td>✔️</td><td>✔️</td><td>✔️</td>
    </tr>
    <tr>
      <td>Hive</td>
      <td>✔️</td><td>✔️</td><td></td><td></td><td>✔️</td>
    </tr>
    <tr>
      <td>HiveServer2</td>
      <td></td><td></td><td></td><td></td><td>✔️</td>
    </tr>
    <tr>
      <td>Flume</td>
      <td></td><td>✔️</td><td></td><td></td><td></td>
    </tr>
    <tr>
      <td>DataX</td>
      <td></td><td>✔️</td><td></td><td></td><td></td>
    </tr>
    <tr>
      <td>Airflow</td>
      <td></td><td>✔️</td><td></td><td></td><td></td>
    </tr>
    <tr>
      <td>Atlas</td>
      <td></td><td>✔️</td><td></td><td></td><td></td>
    </tr>
    <tr>
      <td>Griffin</td>
      <td></td><td>✔️</td><td></td><td></td><td></td>
    </tr>
    <tr>
      <td>Impala</td>
      <td>✔️</td><td>✔️</td><td>✔️</td><td>✔️</td><td>✔️</td>
    </tr>
    <tr>
      <td>MySQL</td>
      <td></td><td>✔️</td><td></td><td></td><td></td>
    </tr>
  </tbody>
</table>

### Hadoop集群配置

> 1. hadoop安装
>
> 2. hadoop集群配置=HDFS集群配置 + MapReduce集群配置 + Yarn集群配置

hadoop01上

#### 第1节 安装hadoop

1. 解压

   ```
   tar -zxvf hadoop-2.9.2.tar.gz -C /opt/servers/
   ```

2. 配置环境变量，在`/etc/profile`文件增加配置

   ```
   export HADOOP_HOME=/opt/servers/hadoop-2.9.2
   export PATH=.:$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   ```

   执行`source /etc/profile`使环境变量生效

3. 验证

  ```
  java -version
  hadoop version
  ```
  

#### 第2节 HDFS集群配置

hadoop01进入配置目录

```
cd $HADOOP_HOME/etc/hadoop
```

1. hadoop-env.sh，将JDK路径明确配置给HDFS

   ```
   export JAVA_HOME=/opt/servers/jdk1.8
   ```

2. 指定NameNode节点为hadoop01

   配置文件为core-site.xml

   ```xml
   <configuration>
       <!-- 指定HDFS中NameNode的地址 -->
       <property>
           <name>fs.defaultFS</name>
           <value>hdfs://hadoop01:9000</value>
       </property>
       <!-- 指定Hadoop运行时产生文件的存储目录 -->
       <property>
           <name>hadoop.tmp.dir</name>
           <value>/opt/servers/hadoop-2.9.2/data/tmp</value>
       </property>
   </configuration>
   ```

3. 指定SecondaryNameNode节点为hadoop02，副本数量为5

   配置文件为hdfs-site.xml

   ```xml
   <configuration>
       <!-- 指定Hadoop辅助名称节点主机配置 -->
       <property>
           <name>dfs.namenode.secondary.http-address</name>
           <value>hadoop02:50090</value>
       </property>
       <!--副本数量 -->
       <property>
           <name>dfs.replication</name>
           <value>5</value>
       </property>
   </configuration>
   ```

4. 制定DataNode从节点

   配置文件为slaves

   ```
   hadoop01
   hadoop02
   hadoop03
   hadoop04
   hadoop05
   ```

#### 第3节 MapReduce集群配置

1. mapred-env.sh，将JDK路径明确配置给MapReduce

   ```
   export JAVA_HOME=/opt/servers/jdk1.8
   ```

2. 指定MapReduce计算框架运行在Yarn资源调度框架上

   ```shell
   mv mapred-site.xml.template mapred-site.xml
   ```

   配置内容

   ```xml
   <configuration>
       <!-- 指定MR运行在Yarn上 -->
       <property>
           <name>mapreduce.framework.name</name>
           <value>yarn</value>
       </property>
   </configuration>
   ```

#### 第4节 Yarn集群配置

1. yarn-env.sh，将JDK路径明确配置给Yarn

   ```
   export JAVA_HOME=/opt/servers/jdk1.8
   ```

2. 指定ResourceManager所在节点为hadoop01

   配置文件yarn-site.xml

   ```
   <configuration>
       <!-- 指定YARN的ResourceManager的地址 -->
       <property>
           <name>yarn.resourcemanager.hostname</name>
           <value>hadoop01</value>
       </property>
       <!-- Reducer获取数据的方式 -->
       <property>
           <name>yarn.nodemanager.aux-services</name>
           <value>mapreduce_shuffle</value>
       </property>
   </configuration>
   ```

3. 指定NodeManager节点（slaves文件已配置）

#### 第5节 文件分发及启动

1. 修改Hadoop安装目录所属用户和用户组

   ```
   chown -R root:root /opt/servers/hadoop-2.9.2
   ```

2. 分发文件

   ```
   rsync-script /opt/servers/hadoop-2.9.2
   rsync-script /etc/profile
   ```

   在集群所有节点执行`source /etc/profile`使环境变量生效

3. namenode初始化

   ```
   hadoop namenode -format
   ```

4. 启动

   在bigdata01上启动dfs

   ```
   start-dfs.sh
   ```

   在bigdata01上启动yarn

   ```
   start-yarn.sh
   ```

   

#### 